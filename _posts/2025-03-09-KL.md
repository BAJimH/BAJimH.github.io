---
title: '生成模型，采样、分布、最大似然估计和KL散度、熵'
date: 2025-03-08
permalink: /posts/2025/03/KL/
tags:
  - learning note
---

### 2025.3.8 生成模型，采样、分布、最大似然估计和KL散度、熵

模型都是人定的，建模就是找到最合适的简单的可得到的玩意，去描述一个无法直接得到的东西。

真实场景是不好得到的，数学建模就是用数学去描述真实世界。



生成模型是什么？

>   有一堆离散的数据样本$\mathcal D =\{x_i\},|\mathcal D|=N$，根据概率的观点，我们认为它符合某种分布$P(x)$  ，但是我们并不知道$P$长什么样子
>
>   目标：我们希望得到若干个$\mathcal D$之外的样本$x'$，它也服从分布$P$

想研究$P$，最直接的办法就是算出$P$概率密度函数的表达式，这意味着只要来一个$x$，就能立刻算出$P(x)$。（**尽管有了概率密度，也不一定很好采样，采样方法也是个很大的题，后面再讨论**）



**第一种办法**：最naive的法子：经验分布，直接估计出$P$
$$
\hat P(x) = {count(x_i)\over N}

$$
缺陷：尽管这是一个对$P$的好像还行的估计，但是它无法估计出数据样本之外的概率——对于其他的$x$，$P(x)$并不一定是$0$，它无法建模数据之间的关联，无法发现数据样本之外的东西，也就是说，描述能力非常有限，用时髦的话来讲就是**毫无泛化性可言**。



更先进一些，假如知道它服从或者**近似服从**某种分布（例如正态、泊松、......），这叫做”**建议分布**“，后面再细细讨论

那么描述$P$就变得比较简单——只需要找到某几个参数，例如正态分布只用找到$\mu,\sigma$就可以了。不妨用$\theta$来统一表示这些可以**参数化**的分布，用来描述$P$的参数化的分布记为$P_\theta$。



问题就变成，在当前的样本$\mathcal D$下，如何估计最好的$\theta$，如何评价为**好**？两种认识思路。

-   极大似然估计：希望在$P_\theta$下，这些$x_i$同时出现的联合概率最大（概率最大的事情最有可能发生，这些数据样本出现（发生）了）。很直接的就有

$$
\theta^*=\mathop{\arg\max}\limits_{\theta}\sum_{i} P_\theta(x_i)
$$

-   $P_\theta$和$P$应该尽可能的像。怎么评价两个分布像不像？一种办法是KL散度
    $$
    KL(P||P_\theta) = \mathbb E_{x\sim P}\log\left(P(x)\over P_\theta(x)\right)
    $$
    回顾一下数学期望的定义，在这里就是枚举所有可能的$x$，它的这个对数值的乘以$P(x)$的和

    即
    $$
    \int_x P(x)\log{P(x)\over P_\theta(x)}
    $$


现在要告诉你，**最大似然估计，和最小化参数分布和经验分布的KL散度，这两个事是等价的**（注意是经验分布，不是真实分布）

这会有几个很自然的疑惑

-   KL散度到底是个什么玩意？
-   为什么这玩意能描述两个分布的差异？
-   $\log$是哪里来的？

要说清楚KL散度，得回到信息论的概念里头。

信息被定义为“消除不确定性的东西”。如何定义信息量？信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息。这个多和少，和事件发生的概率有关。

• 非常可能发生的事件信息量要比较少，并且极端情况下，百分百能够发生的事件应该没有信息量。
• 较不可能发生的事件具有更高的信息量。
• 独立事件应具有增量的信息（可加性）。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。

那么满足这些条件的函数可以写成
$$
I(x) = -\log P(x)
$$
这是对于单个服从分布$P$的随机事件的定义。对于多个随机事件，信息量应该是可以相加的，那么整个分布的信息量就是
$$
\mathbb E_{x\sim P}I(x) = \mathbb E_{x\sim P}-\log P(x) = \int_x -P(x) \log P(x)\mathrm dx
$$
对于离散情况下，把积分换成求和
$$
\sum_x -P(x) \log P(x)
$$
这个东西就定义为**熵**，记作$H(P)$

熵还有另外一种解释，**最小平均编码长度**。对于一个服从分布$P$的随机事件，要将每个事件用二进制前缀编码，如果用最优的哈夫曼编码方式，平均每个事件的编码长度为$H(P)$。证明略

回到原来的问题，这样就解释了$\log$的来源。从信息论的角度，KL散度可以写成
$$
KL(P||P_\theta) = -H(P)+ \int_x -P(x)\log P_\theta(x)\mathrm dx
$$

后一项，可以看作是（我根据$P_\theta$的大小，对$P$进行编码，但是真实分布是$P$，平均编码长度），这就是**交叉熵** $H(P,P_\theta)$。
所以KL散度等于交叉熵减去$P$的熵，也叫**相对熵**。直观上，交叉熵或者相对熵越小，$P$和$P_\theta$就越像。

我们最后用数学说明KL散度的性质（很明显**它不满足对称性**）。这个东西也叫吉布斯不等式。

-   $KL(P||P_\theta)\geq 0$，等号成立当且仅当$P=P_\theta$

注意到$-\log x\geq 1-x$，所以
$$
\int_x P(x)\log {P(x) \over P_\theta(x)}\mathrm dx \geq \int_x P(x)\left({P_\theta(x) \over P(x)}-1\right)\mathrm dx = \int_x P_\theta(x)\mathrm dx - \int_x P(x)\mathrm dx = 1-1=0 （概率和为1）
$$

最后回到正题，极大似然估计
$$
\theta^*=\mathop{\arg\max}\limits_{\theta}\sum_{i} P_\theta(x_i)
 = \mathop{\arg\max}\limits_{\theta}\sum_{i} \log P_\theta(x_i)
$$

由于$P$的熵与$\theta$无关，所以最小化$P$和$P_\theta$的KL散度等价于最小化它们的交叉熵

$$
\theta^*=\mathop{\arg\min}\limits_{\theta}KL(P||P_\theta) = \mathop{\arg\min}\limits_{\theta}H(P,P_\theta) = \mathop{\arg\max}\limits_{\theta}\sum_x P(x)\log P_\theta(x)(取了一下反)
$$

由于$\mathcal D$是从$P$中采样出来的，只能用经验分布$\hat P(x) = {count(x_i)\over N}$来估计$P(x)$，所以$\sum_x P(x)\log P_\theta(x)$可以写成
$$
\sum_{x\in\mathcal D} \log P_\theta(x)
$$

完事。
